---
page_title: "ably_rule_kafka Resource - terraform-provider-ably"
subcategory: ""
description: |-
  The ably_rule_kafka resource allows you to create and manage an Ably integration rule for Kafka. Read more at https://ably.com/docs/general/firehose/kafka-rule
---

# ably_rule_kafka (Resource)

The `ably_rule_kafka` resource allows you to create and manage an Ably integration rule for Kafka. Read more at https://ably.com/docs/general/firehose/kafka-rule


## Example Usage

```terraform
resource "ably_rule_kafka" "rule0" {
  app_id = ably_app.app0.id
  status = "enabled"
  source = {
    channel_filter = "^my-channel.*",
    type           = "channel.message"
  }
  target = {
    routing_key = "topic:key",
    brokers     = ["kafka.ci.ably.io:19092", "kafka.ci.ably.io:19093"]
    auth = {
      sasl = {
        mechanism = "scram-sha-256"
        username  = "username"
        password  = "password"
      }
    }
    enveloped = true
    format    = "json"
  }
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `app_id` (String) The Ably application ID.
- `source` (Attributes) object (rule_source) (see [below for nested schema](#nestedatt--source))
- `target` (Attributes) object (rule_source) (see [below for nested schema](#nestedatt--target))

### Optional

- `request_mode` (String) This is Single Request mode or Batch Request mode. Single Request mode sends each event separately to the endpoint specified by the rule
- `status` (String) The status of the rule. Rules can be enabled or disabled.

### Read-Only

- `id` (String) The rule ID.

<a id="nestedatt--source"></a>
### Nested Schema for `source`

Required:

- `type` (String)

Optional:

- `channel_filter` (String)


<a id="nestedatt--target"></a>
### Nested Schema for `target`

Required:

- `auth` (Attributes) The Kafka [authentication mechanism](https://docs.confluent.io/platform/current/kafka/overview-authentication-methods.html) (see [below for nested schema](#nestedatt--target--auth))
- `brokers` (List of String) This is a list of brokers that host your Kafka partitions. Each broker is specified using the format `host`, `host:port` or `ip:port`
- `routing_key` (String) The Kafka partition key. This is used to determine which partition a message should be routed to, where a topic has been partitioned. routingKey should be in the format topic:key where topic is the topic to publish to, and key is the value to use as the message key

Optional:

- `enveloped` (Boolean) Delivered messages are wrapped in an Ably envelope by default that contains metadata about the message and its payload. The form of the envelope depends on whether it is part of a Webhook/Function or a Queue/Firehose rule. For everything besides Webhooks, you can ensure you only get the raw payload by unchecking "Enveloped" when setting up the rule.
- `format` (String) JSON provides a text-based encoding, whereas MsgPack provides a more efficient binary encoding

<a id="nestedatt--target--auth"></a>
### Nested Schema for `target.auth`

Optional:

- `sasl` (Attributes) SASL(Simple Authentication Security Layer) / SCRAM (Salted Challenge Response Authentication Mechanism) uses usernames and passwords stored in ZooKeeper. Credentials are created during installation. See documentation on [configuring SCRAM](https://docs.confluent.io/platform/current/kafka/authentication_sasl/authentication_sasl_scram.html#kafka-sasl-auth-scram) (see [below for nested schema](#nestedatt--target--auth--sasl))

<a id="nestedatt--target--auth--sasl"></a>
### Nested Schema for `target.auth.sasl`

Required:

- `mechanism` (String) `plain` `scram-sha-256` `scram-sha-512`. The hash type to use. SCRAM supports either SHA-256 or SHA-512 hash functions
- `password` (String, Sensitive) Kafka login credential
- `username` (String, Sensitive) Kafka login credential